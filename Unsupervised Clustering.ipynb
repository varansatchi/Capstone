{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('./russian-troll-tweets/IRAhandle_tweets_1.csv')\n",
    "df2 = pd.read_csv('./russian-troll-tweets/IRAhandle_tweets_2.csv')\n",
    "df3 = pd.read_csv('./russian-troll-tweets/IRAhandle_tweets_3.csv')\n",
    "df4 = pd.read_csv('./russian-troll-tweets/IRAhandle_tweets_4.csv')\n",
    "df5 = pd.read_csv('./russian-troll-tweets/IRAhandle_tweets_5.csv')\n",
    "df6 = pd.read_csv('./russian-troll-tweets/IRAhandle_tweets_6.csv')\n",
    "df7 = pd.read_csv('./russian-troll-tweets/IRAhandle_tweets_7.csv')\n",
    "df8 = pd.read_csv('./russian-troll-tweets/IRAhandle_tweets_8.csv')\n",
    "df9 = pd.read_csv('./russian-troll-tweets/IRAhandle_tweets_9.csv')\n",
    "df10 = pd.read_csv('./russian-troll-tweets/IRAhandle_tweets_10.csv')\n",
    "df11 = pd.read_csv('./russian-troll-tweets/IRAhandle_tweets_11.csv')\n",
    "df12 = pd.read_csv('./russian-troll-tweets/IRAhandle_tweets_12.csv')\n",
    "df13 = pd.read_csv('./russian-troll-tweets/IRAhandle_tweets_13.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [df1, df2, df3, df4, df5, df6, df7, df8, df9, df10, df11, df12, df13]\n",
    "master = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = master['language'] == 'English'\n",
    "eng = master[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_df = eng['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_df = content_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_df = content_df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizing, Stemming, and Tokenzing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "punc = ['.', ',', '\"', \"'\", '?', '!', ':', ';', '(', ')', '[', ']', '{', '}',\"%\"]\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(punc)\n",
    "desc = content_df.values\n",
    "vectorizer = TfidfVectorizer(stop_words = stop_words)\n",
    "X = vectorizer.fit_transform(desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2378221\n"
     ]
    }
   ],
   "source": [
    "word_features = vectorizer.get_feature_names()\n",
    "print(len(word_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('english')\n",
    "tokenizer = RegexpTokenizer(r'[a-zA-Z\\']+')\n",
    "\n",
    "def tokenize(text):\n",
    "    return [stemmer.stem(word) for word in tokenizer.tokenize(text.lower())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2050047\n",
      "[\"''\", \"''twas\", \"'a\", \"'aaron'\", \"'abov\", \"'adopts'\", \"'advic\", \"'b\", \"'bama\", \"'bird\", \"'bless\", \"'bori\", \"'bout\", \"'bronx\", \"'c\", \"'china\", \"'d\", \"'dear\", \"'discoveref\", \"'duck\", \"'e\", \"'ebtih\", \"'em\", \"'energizer'\", \"'f\", \"'fences'\", \"'forev\", \"'g\", \"'guy\", \"'h\", \"'hand\", \"'happi\", \"'hate\", \"'he\", \"'he'd\", \"'healthi\", \"'heeey\", \"'henri\", \"'honesti\", \"'how\", \"'i\", \"'i'm\", \"'if\", \"'increas\", \"'is\", \"'islamophobia\", \"'it\", \"'j\", \"'justic\", \"'k\"]\n"
     ]
    }
   ],
   "source": [
    "vectorizer2 = TfidfVectorizer(stop_words = stop_words, tokenizer = tokenize)\n",
    "X2 = vectorizer2.fit_transform(desc)\n",
    "word_features2 = vectorizer2.get_feature_names()\n",
    "print(len(word_features2))\n",
    "print(word_features2[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer3 = TfidfVectorizer(stop_words = stop_words, tokenizer = tokenize, max_features = 1000)\n",
    "X3 = vectorizer3.fit_transform(desc)\n",
    "words = vectorizer3.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : https, t, rt, polit, peopl, like, just, local, amp, say, new, black, obama, don't, man, polic, i'm, make, time, year, love, need, hillari, day, know\n",
      "1 : s, https, t, u, trump, break, obama, new, video, just, hillari, n, http, o, k, m, topnew, c, d, p, news, e, h, f, b\n",
      "2 : http, t, workout, exercis, weight, lose, need, good, k, m, fit, d, j, https, r, b, c, x, e, n, l, diet, p, g, u\n",
      "3 : sport, win, game, open, warrior, lsu, final, nfl, cleveland, coach, s, new, lead, beat, state, say, team, player, season, footbal, report, star, miss, bowl, play\n",
      "4 : news, world, t, say, https, kill, polic, s, u, state, new, local, man, attack, china, report, suspect, arrest, year, syria, dead, shoot, islam, south, eu\n",
      "5 : trump, https, t, donald, presid, polit, support, rt, just, break, video, say, obama, clinton, anti, vote, media, hillari, look, maga, gop, new, attack, elect, liber\n",
      "6 : want, z, https, t, don't, rt, peopl, just, http, trump, know, man, like, say, make, hillari, u, amp, new, polic, n, m, w, whi, b\n",
      "7 : https, t, polic, man, new, h, d, k, m, n, e, c, p, w, f, b, o, g, x, r, l, v, y, j, q\n"
     ]
    }
   ],
   "source": [
    "kmeans = KMeans(n_clusters = 8, n_init = 20, n_jobs = 1)\n",
    "kmeans.fit(X3)\n",
    "# Going to use 8 clusters to replicate the 8 category types the original Clemson researchers classified\n",
    "common_words = kmeans.cluster_centers_.argsort()[:,-1:-26:-1]\n",
    "for num, centroid in enumerate(common_words):\n",
    "    print(str(num) + ' : ' + ', '.join(words[word] for word in centroid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
